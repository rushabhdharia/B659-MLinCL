{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. http://www.cai.sk/ojs/index.php/cai/article/download/82/66\n",
    "2. http://www.aclweb.org/anthology/C16-1189\n",
    "3. https://pdfs.semanticscholar.org/57e1/eeb8e4f442f4433670b50167404c14566e97.pdf\n",
    "\n",
    "## Code Referred from\n",
    "1. https://machinelearnings.co/text-classification-using-neural-networks-f5cd7b8765c6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "aeRgzp2SIK1B",
    "outputId": "bbcb8c6c-4b46-4ac6-9274-045cf2922914"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rushabh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import json\n",
    "import datetime\n",
    "stemmer = LancasterStemmer()\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used 3 classes of data - greeting, goodbye and request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HOwvhNtMIQq-",
    "outputId": "7aa513cb-c92b-46f1-bbb8-c0376dbb3851"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 sentences in training data\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"how are you?\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"how is your day?\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"good day\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"how is it going today?\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"Hello\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"Hi\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"Greetings!\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"Hi, How is it going?\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"How are you doing?\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"Nice to meet you.\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"How do you do?\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"Hi, nice to meet you.\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"how is your day?\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"good day\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"It is a pleasure to meet you.\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"Top of the morning to you!\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"Hey!\"})\n",
    "\n",
    "\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"have a nice day\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"see you later\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"have a nice day\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"talk to you soon\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"See you later, alligator!\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"After a while, crocodile.\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"Stay out of trouble.\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"I’m out of here.\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"Okay...bye, fry guy!\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"If I don’t see you around, I'll see you square.\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"Stay classy.\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"Fare thee well.\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"Catch you on the rebound.\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"Gotta go, buffalo.\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"Peace out!\"})\n",
    "training_data.append({\"class\":\"goodbye\", \"sentence\":\"Gotta hit the road.\"})\n",
    "\n",
    "training_data.append({\"class\":\"request\", \"sentence\":\"Could you give please? \"})\n",
    "training_data.append({\"class\":\"request\", \"sentence\":\"Can you give please? \"})\n",
    "training_data.append({\"class\":\"request\", \"sentence\":\"Do you think you could lend me some money?\"})\n",
    "training_data.append({\"class\":\"request\", \"sentence\":\"can you make a sandwich?\"})\n",
    "training_data.append({\"class\":\"request\", \"sentence\":\"I wonder whether you could give me a car?\"})\n",
    "training_data.append({\"class\":\"request\", \"sentence\":\"I am sorry to trouble you but I need your help?\"})\n",
    "training_data.append({\"class\":\"request\", \"sentence\":\"I hope you don't mind if l asked the money?\"})\n",
    "training_data.append({\"class\":\"request\", \"sentence\":\"Would you mind if I ask your help?\"})\n",
    "training_data.append({\"class\":\"request\", \"sentence\":\"I request you to grant me leave?\"})\n",
    "training_data.append({\"class\":\"request\", \"sentence\":\"How about giving your car please?\"})\n",
    "training_data.append({\"class\":\"request\", \"sentence\":\"Kindly allow me to talk?\"})\n",
    "training_data.append({\"class\":\"request\", \"sentence\":\"Please let me know if you come or not?\"})\n",
    "training_data.append({\"class\":\"request\", \"sentence\":\"Can you meet me tomorrow?\"})\n",
    "\n",
    "print (\"%s sentences in training data\" % len(training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word in every sentence is tokenized and stemmed and lower-cased and a list of words is generated.  \n",
    "classes list holds all the classes  \n",
    "documents list holds all the tokenized words and the class the text belongs to.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "0z4eNNgjIZGA",
    "outputId": "00a375c1-9f95-4da6-ca31-2f3017655822"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 documents\n",
      "3 classes ['goodbye', 'greeting', 'request']\n",
      "104 unique stemmed words ['okay', 'would', 'talk', 'catch', 'on', 'classy', ',', 'it', 'going', 'crocodil', '!', '.', 'wel', 'morn', 'fry', 'see', 'troubl', 'giv', 'pleas', 'whil', 'me', 'can', 't', 'allig', '’', 'i', 'request', 'hello', 'peac', 'her', 'the', 'kind', 'nee', 'wond', 'help', 'car', 'wheth', 'if', 'not', \"n't\", 'hey', 'hav', 'today', 'tomorrow', 'you', 'know', \"'ll\", 'out', 'but', 'grant', 'or', 'could', 'guy', 'a', 'to', 'com', 'let', 'aft', 'leav', 'am', 'about', 'got', 'sorry', 'meet', 'squ', 'soon', 'around', 'greet', 'ta', 'yo', 'ar', 'think', 'hit', 'lat', 'mind', 'do', 'mak', 'rebound', 'money', 'of', 'far', 'is', 'l', 'doing', 'stay', 'bye', 'allow', 'road', 'm', 'lend', 'top', 'hi', 'sandwich', 'good', 'how', '...', 'don', 'day', 'ask', 'buffalo', 'nic', 'som', 'go', 'hop']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "\n",
    "for pattern in training_data:\n",
    "    w = nltk.word_tokenize(pattern['sentence'])\n",
    "    words.extend(w)\n",
    "    documents.append((w, pattern['class']))\n",
    "    if pattern['class'] not in classes:\n",
    "        classes.append(pattern['class'])\n",
    "\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = list(set(words))\n",
    "\n",
    "classes = list(set(classes))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is transformed into bag-of-words for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "HZ1QgfysIluC",
    "outputId": "0c3d7287-1d23-4dd3-e204-196127c79c02"
   },
   "outputs": [],
   "source": [
    "training = []\n",
    "output = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    pattern_words = doc[0]\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    training.append(bag)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    output.append(output_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid Function is used to normalize values.  \n",
    "It's derivative is used to measure error rate.  \n",
    "\n",
    "bow() is used to vectorize a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2fvnB_fIvHy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    " \n",
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "def bow(sentence, words, show_details=False):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))\n",
    "\n",
    "def think(sentence, show_details=False):\n",
    "    x = bow(sentence.lower(), words, show_details)\n",
    "    if show_details:\n",
    "        print (\"sentence:\", sentence, \"\\n bow:\", x)\n",
    "    l0 = x\n",
    "    l1 = sigmoid(np.dot(l0, synapse_0))\n",
    "    l2 = sigmoid(np.dot(l1, synapse_1))\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network training function to create synaptic weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iSyXzdkUI1aZ"
   },
   "outputs": [],
   "source": [
    "def train(X, y, hidden_neurons=10, alpha=1, epochs=50000, dropout=False, dropout_percent=0.5):\n",
    "\n",
    "    print (\"Training with %s neurons, alpha:%s, dropout:%s %s\" % (hidden_neurons, str(alpha), dropout, dropout_percent if dropout else '') )\n",
    "    print (\"Input matrix: %sx%s    Output matrix: %sx%s\" % (len(X),len(X[0]),1, len(classes)) )\n",
    "    np.random.seed(1)\n",
    "\n",
    "    last_mean_error = 1\n",
    "    synapse_0 = 2*np.random.random((len(X[0]), hidden_neurons)) - 1\n",
    "    synapse_1 = 2*np.random.random((hidden_neurons, len(classes))) - 1\n",
    "\n",
    "    prev_synapse_0_weight_update = np.zeros_like(synapse_0)\n",
    "    prev_synapse_1_weight_update = np.zeros_like(synapse_1)\n",
    "\n",
    "    synapse_0_direction_count = np.zeros_like(synapse_0)\n",
    "    synapse_1_direction_count = np.zeros_like(synapse_1)\n",
    "        \n",
    "    for j in iter(range(epochs+1)):\n",
    "\n",
    "        layer_0 = X\n",
    "        layer_1 = sigmoid(np.dot(layer_0, synapse_0))\n",
    "                \n",
    "        if(dropout):\n",
    "            layer_1 *= np.random.binomial([np.ones((len(X),hidden_neurons))],1-dropout_percent)[0] * (1.0/(1-dropout_percent))\n",
    "\n",
    "        layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
    "\n",
    "        layer_2_error = y - layer_2\n",
    "\n",
    "        if (j% 10000) == 0 and j > 5000:\n",
    "            if np.mean(np.abs(layer_2_error)) < last_mean_error:\n",
    "                print (\"delta after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error))) )\n",
    "                last_mean_error = np.mean(np.abs(layer_2_error))\n",
    "            else:\n",
    "                print (\"break:\", np.mean(np.abs(layer_2_error)), \">\", last_mean_error )\n",
    "                break\n",
    "                \n",
    "        layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2)\n",
    "\n",
    "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
    "\n",
    "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "        \n",
    "        synapse_1_weight_update = (layer_1.T.dot(layer_2_delta))\n",
    "        synapse_0_weight_update = (layer_0.T.dot(layer_1_delta))\n",
    "        \n",
    "        if(j > 0):\n",
    "            synapse_0_direction_count += np.abs(((synapse_0_weight_update > 0)+0) - ((prev_synapse_0_weight_update > 0) + 0))\n",
    "            synapse_1_direction_count += np.abs(((synapse_1_weight_update > 0)+0) - ((prev_synapse_1_weight_update > 0) + 0))        \n",
    "        \n",
    "        synapse_1 += alpha * synapse_1_weight_update\n",
    "        synapse_0 += alpha * synapse_0_weight_update\n",
    "        \n",
    "        prev_synapse_0_weight_update = synapse_0_weight_update\n",
    "        prev_synapse_1_weight_update = synapse_1_weight_update\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    synapse = {'synapse0': synapse_0.tolist(), 'synapse1': synapse_1.tolist(),\n",
    "               'datetime': now.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "               'words': words,\n",
    "               'classes': classes\n",
    "              }\n",
    "    synapse_file = \"synapses.json\"\n",
    "\n",
    "    with open(synapse_file, 'w') as outfile:\n",
    "        json.dump(synapse, outfile, indent=4, sort_keys=True)\n",
    "    print (\"saved synapses to:\", synapse_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "kFq5gOW4I8BD",
    "outputId": "8afa8dc5-4d5f-43b2-d318-357ecc53c1e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 20 neurons, alpha:0.1, dropout:False \n",
      "Input matrix: 46x104    Output matrix: 1x3\n",
      "delta after 10000 iterations:0.003515204506123834\n",
      "delta after 20000 iterations:0.0024040974715811986\n",
      "delta after 30000 iterations:0.001929927866145538\n",
      "delta after 40000 iterations:0.0016528880756525683\n",
      "delta after 50000 iterations:0.0014663625439096794\n",
      "delta after 60000 iterations:0.0013300521669967935\n",
      "delta after 70000 iterations:0.0012249499565234287\n",
      "delta after 80000 iterations:0.00114078087650855\n",
      "delta after 90000 iterations:0.001071446207319715\n",
      "delta after 100000 iterations:0.001013070062454832\n",
      "saved synapses to: synapses.json\n",
      "processing time: 9.495219707489014 seconds\n"
     ]
    }
   ],
   "source": [
    "X = np.array(training)\n",
    "y = np.array(output)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train(X, y, hidden_neurons=20, alpha=0.1, epochs=100000, dropout=False, dropout_percent=0.2)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print (\"processing time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 969
    },
    "colab_type": "code",
    "id": "3NJiMSKUJC5k",
    "outputId": "9ad8a6a5-bcd4-486b-9da6-0d223bce9207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's up? \n",
      " classification: [['greeting', 0.2353775939377253]]\n",
      "how is your day? \n",
      " classification: [['greeting', 0.9995292054063607]]\n",
      "good day \n",
      " classification: [['greeting', 0.9985716221793971]]\n",
      "Hey, How are you? \n",
      " classification: [['greeting', 0.9997715779036666]]\n",
      "how is it going today? \n",
      " classification: [['greeting', 0.9998395464258742]]\n",
      "Long live and prosper! \n",
      " classification: [['greeting', 0.8831475515075958]]\n",
      "Well, I'm off! \n",
      " classification: [['goodbye', 0.9264700751964665]]\n",
      "Smoke me a kipper, I'll be back for breakfast. \n",
      " classification: [['request', 0.9283379531927487], ['goodbye', 0.5509899614289613]]\n",
      "Long live and prosper! \n",
      " classification: [['greeting', 0.8831475515075958]]\n",
      "Well, I'm off! \n",
      " classification: [['goodbye', 0.9264700751964665]]\n",
      "Smoke me a kipper, I'll be back for breakfast. \n",
      " classification: [['request', 0.9283379531927487], ['goodbye', 0.5509899614289613]]\n",
      "Bye bye, butterfly. \n",
      " classification: [['goodbye', 0.9889625187255977]]\n",
      "Gotta get going. \n",
      " classification: [['goodbye', 0.9971646914974251]]\n",
      "how are you today? \n",
      " classification: [['greeting', 0.9941197027429697]]\n",
      "talk to you tomorrow \n",
      " classification: []\n",
      "who are you? \n",
      " classification: [['greeting', 0.8620276042955236]]\n",
      "make me some lunch \n",
      " classification: [['request', 0.995623192425521]]\n",
      "how was your lunch today? \n",
      " classification: [['greeting', 0.6336435563025845]]\n",
      "Can you do this? \n",
      " classification: [['request', 0.83074210664038]]\n",
      "Can you meet me at 5 pm? \n",
      " classification: [['request', 0.9981019920877328]]\n",
      "Please come and see me in my office at 4! \n",
      " classification: [['request', 0.696190404330081]]\n",
      "Can you bring my coat? \n",
      " classification: [['request', 0.8389597803205885]]\n",
      "Can you repay me the amount you owe? \n",
      " classification: [['request', 0.9960621986605448]]\n",
      "\n",
      "found in bag: good\n",
      "found in bag: day\n",
      "sentence: good day \n",
      " bow: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0]\n",
      "good day \n",
      " classification: [['greeting', 0.9985716221793971]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['greeting', 0.9985716221793971]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ERROR_THRESHOLD = 0.2\n",
    "synapse_file = 'synapses.json' \n",
    "with open(synapse_file) as data_file: \n",
    "    synapse = json.load(data_file) \n",
    "    synapse_0 = np.asarray(synapse['synapse0']) \n",
    "    synapse_1 = np.asarray(synapse['synapse1'])\n",
    "\n",
    "def classify(sentence, show_details=False):\n",
    "    results = think(sentence, show_details)\n",
    "\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD ] \n",
    "    results.sort(key=lambda x: x[1], reverse=True) \n",
    "    return_results =[[classes[r[0]],r[1]] for r in results]\n",
    "    print (\"%s \\n classification: %s\" % (sentence, return_results))\n",
    "    return return_results\n",
    "\n",
    "classify(\"What's up?\")\n",
    "classify(\"how is your day?\")\n",
    "classify(\"good day\")\n",
    "classify(\"Hey, How are you?\")\n",
    "classify(\"how is it going today?\")\n",
    "classify(\"Long live and prosper!\")\n",
    "classify(\"Well, I'm off!\")\n",
    "classify(\"Smoke me a kipper, I'll be back for breakfast.\")\n",
    "classify(\"Long live and prosper!\")\n",
    "classify(\"Well, I'm off!\")\n",
    "classify(\"Smoke me a kipper, I'll be back for breakfast.\")\n",
    "classify(\"Bye bye, butterfly.\")\n",
    "classify(\"Gotta get going.\")\n",
    "classify(\"how are you today?\")\n",
    "classify(\"talk to you tomorrow\")\n",
    "classify(\"who are you?\")\n",
    "classify(\"make me some lunch\")\n",
    "classify(\"how was your lunch today?\")\n",
    "classify(\"Can you do this?\")\n",
    "classify(\"Can you meet me at 5 pm?\")\n",
    "classify(\"Please come and see me in my office at 4!\")\n",
    "classify(\"Can you bring my coat?\")\n",
    "classify(\"Can you repay me the amount you owe?\")\n",
    "\n",
    "\n",
    "print()\n",
    "classify(\"good day\", show_details=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach: A basic bag-of-words model is used to train on a 2 Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas for Improvement  \n",
    "1. Removing Stop Words\n",
    "2. Removing Noise from data\n",
    "3. Using features like Ngrams and POS tags\n",
    "4. Remove all punctuations\n",
    "5. Use RNN or LSTM\n",
    "6. Could use a tree based classifier like Random Forests"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_assignment_2 (1).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
