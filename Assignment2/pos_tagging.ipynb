{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9eLyiuIBsk_v"
   },
   "source": [
    "# References\n",
    "1. http://www.aclweb.org/anthology/C94-1027\n",
    "2. https://becominghuman.ai/part-of-speech-tagging-tutorial-with-the-keras-deep-learning-library-d7f93fa05537"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hDu6IaOqsk_x"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "uWqD5H7Nsk_2",
    "outputId": "c98fe3e0-3540-4bad-ad35-83647467d46a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/rushabh/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/rushabh/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3mw1aAPAsk_7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3914"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "sentences = treebank.tagged_sents(tagset='universal')\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80% of data for training, 20% for testing.  \n",
    "25% of training data used as validation set  \n",
    "75% of training data used as training set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tLUkArVbslAC"
   },
   "outputs": [],
   "source": [
    "train_test_cutoff = int(.80 * len(sentences)) \n",
    "training_sentences = sentences[:train_test_cutoff]\n",
    "testing_sentences = sentences[train_test_cutoff:]\n",
    " \n",
    "train_val_cutoff = int(.25 * len(training_sentences))\n",
    "validation_sentences = training_sentences[:train_val_cutoff]\n",
    "training_sentences = training_sentences[train_val_cutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary of features is created   \n",
    "Features  \n",
    "1. checks if the term(word) is first in the sentence  \n",
    "2. checks if the term(word) is last in the sentence  \n",
    "3. 2 and 3. letter prefixes  \n",
    "4. 2 and 3 letter suffixes  \n",
    "5. previous and next words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pxJVifkHslAF"
   },
   "outputs": [],
   "source": [
    "def add_basic_features(sentence_terms, index):\n",
    "    term = sentence_terms[index]\n",
    "    return {\n",
    "        'term': term,\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence_terms) - 1,\n",
    "        'prefix-2': term[:2],\n",
    "        'prefix-3': term[:3],\n",
    "        'suffix-2': term[-2:],\n",
    "        'suffix-3': term[-3:],,\n",
    "        'prev_word': '' if index == 0 else sentence_terms[index - 1],\n",
    "        'next_word': '' if index == len(sentence_terms) - 1 else sentence_terms[index + 1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "untag() is used to remove the tag associated with each word in a sentence.   \n",
    "transform_to_dataset() generates the input and output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lwWgzOYzslAH"
   },
   "outputs": [],
   "source": [
    "def untag(tagged_sentence):\n",
    "    return [w for w, _ in tagged_sentence]\n",
    "\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    "    for pos_tags in tagged_sentences:\n",
    "        for index, (term, class_) in enumerate(pos_tags):\n",
    "            X.append(add_basic_features(untag(pos_tags), index))\n",
    "            y.append(class_)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VG4iIF8VslAJ"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = transform_to_dataset(training_sentences)\n",
    "X_test, y_test = transform_to_dataset(testing_sentences)\n",
    "X_val, y_val = transform_to_dataset(validation_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "oAnhd9GYslAN",
    "outputId": "d536369b-2914-429d-a282-138c01c69cb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    " \n",
    "dict_vectorizer = DictVectorizer(sparse=False)\n",
    "dict_vectorizer.fit(X_train + X_test + X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used to convert the list of dictionary of elements to a vector as shown here https://stackoverflow.com/questions/27473957/understanding-dictvectorizer-in-scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ojDb9Am9slAS"
   },
   "outputs": [],
   "source": [
    "X_train = dict_vectorizer.transform(X_train)\n",
    "X_test = dict_vectorizer.transform(X_test)\n",
    "X_val = dict_vectorizer.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LabelEncoder is used for Encoding each Part-of-Speech Label with a number.  \n",
    "Reference - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FwMbMPWcslAU",
    "outputId": "7450359d-cc6e-4e38-9226-c73ad1bdd764"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train + y_test + y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QUl2py-lslAX"
   },
   "outputs": [],
   "source": [
    "y_train = label_encoder.transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "y_val = label_encoder.transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RUO0TbYMslAa",
    "outputId": "e1c83149-10a0-4612-966d-eadfef45ad44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    " \n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "y_val = np_utils.to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 3 layer Fully Connected Perceptron is used as mentioned in Schmid's paper - http://www.aclweb.org/anthology/C94-1027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_M5Qr7J1slAe"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "def build_model(input_dim, hidden_neurons, output_dim):\n",
    "    model = Sequential([\n",
    "        Dense(hidden_neurons, input_dim=input_dim),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(hidden_neurons),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g3dPjxQXslAi"
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "model_params = {\n",
    "    'build_fn': build_model,\n",
    "    'input_dim': X_train.shape[1],\n",
    "    'hidden_neurons': 512,\n",
    "    'output_dim': y_train.shape[1],\n",
    "    'epochs': 5,\n",
    "    'batch_size': 256,\n",
    "    'verbose': 1,\n",
    "    'validation_data': (X_val, y_val),\n",
    "    'shuffle': True\n",
    "}\n",
    "\n",
    "clf = KerasClassifier(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "XlGGR-WpslAk",
    "outputId": "957406f8-174a-4fc2-b1e5-a4c86190c56e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 61107 samples, validate on 19530 samples\n",
      "Epoch 1/5\n",
      "61107/61107 [==============================] - 42s 690us/step - loss: 0.3419 - acc: 0.8970 - val_loss: 0.1354 - val_acc: 0.9546\n",
      "Epoch 2/5\n",
      "61107/61107 [==============================] - 26s 426us/step - loss: 0.0490 - acc: 0.9839 - val_loss: 0.1457 - val_acc: 0.9565\n",
      "Epoch 3/5\n",
      "61107/61107 [==============================] - 26s 428us/step - loss: 0.0225 - acc: 0.9925 - val_loss: 0.1449 - val_acc: 0.9603\n",
      "Epoch 4/5\n",
      "61107/61107 [==============================] - 26s 427us/step - loss: 0.0161 - acc: 0.9949 - val_loss: 0.1539 - val_acc: 0.9590\n",
      "Epoch 5/5\n",
      "61107/61107 [==============================] - 26s 427us/step - loss: 0.0118 - acc: 0.9963 - val_loss: 0.1615 - val_acc: 0.9581\n"
     ]
    }
   ],
   "source": [
    "hist = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wuSo1wAIslAs",
    "outputId": "a6a2dec4-bdd5-47a7-dc36-e687f1e489a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy: 0.9656669494485752\n"
     ]
    }
   ],
   "source": [
    "score = clf.score(X_test, y_test, verbose=0)    \n",
    "print('model accuracy: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison to Schmid's implementation\n",
    "1. Uses 6 gram model(preceding 3+ following 2 + word).  \n",
    "I used 3 (1 preceding + word + 1 succeding) as I get almost the same accuracy(96.6%) as shown above.\n",
    "\n",
    "2. I also check if the word is first or last in the sentence.\n",
    "\n",
    "3. Instead of creating a prefix / suffix tree and checking if a particular prefix or suffix exists I consider 2 and 3. letter prefixes\n",
    "2 and 3 letter suffixes for every sentence and encode it in the vector"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pos_tagging_neural_nets_keras.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
