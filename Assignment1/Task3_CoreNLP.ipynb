{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code referred from https://stanfordnlp.github.io/CoreNLP/tutorials.html  \n",
    "Vectorizing Part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanfordNLP:\n",
    "    #Need to run the StanfordCoreNLP server first.\n",
    "    def __init__(self, host='http://localhost', port=9000):\n",
    "        self.nlp = StanfordCoreNLP(host, port=port, timeout=30000)\n",
    "        self.props = {\n",
    "            'annotators': 'tokenize,ssplit,pos,lemma,ner',\n",
    "            'pipelineLanguage': 'en',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "\n",
    "    def word_tokenize(self, sentence):\n",
    "        return self.nlp.word_tokenize(sentence)\n",
    "\n",
    "    def pos(self, sentence):\n",
    "        return self.nlp.pos_tag(sentence)\n",
    "\n",
    "    def ner(self, sentence):\n",
    "        return self.nlp.ner(sentence)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        return self.nlp.parse(sentence)\n",
    "\n",
    "    def dependency_parse(self, sentence):\n",
    "        return self.nlp.dependency_parse(sentence)\n",
    "\n",
    "    def annotate(self, sentence):\n",
    "        return json.loads(self.nlp.annotate(sentence, properties=self.props))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'NNP'), ('met', 'VBD'), ('Susan', 'NNP'), ('in', 'IN'), ('the', 'DT'), ('mall', 'NN'), ('.', '.'), ('She', 'PRP'), ('told', 'VBD'), ('him', 'PRP'), ('that', 'IN'), ('she', 'PRP'), ('is', 'VBZ'), ('traveling', 'VBG'), ('to', 'TO'), ('Europe', 'NNP'), ('next', 'IN'), ('week', 'NN'), ('.', '.')]\n",
      "['John', 'met', 'Susan', 'in', 'the', 'mall', '.', 'She', 'told', 'him', 'that', 'she', 'is', 'traveling', 'to', 'Europe', 'next', 'week', '.']\n"
     ]
    }
   ],
   "source": [
    "sNLP = StanfordNLP()\n",
    "text = 'John met Susan in the mall. She told him that she is traveling to Europe next week.'\n",
    "\n",
    "json = sNLP.annotate(text)\n",
    "pos = sNLP.pos(text)\n",
    "tokens = sNLP.word_tokenize(text)\n",
    "ner = sNLP.ner(text)\n",
    "parse = sNLP.parse(text)\n",
    "dep_parse = sNLP.dependency_parse(text)\n",
    "\n",
    "\n",
    "# print(json)\n",
    "print(pos)\n",
    "print(tokens)\n",
    "# print(ner)\n",
    "# print(parse)\n",
    "# print(dep_parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used the tags mentioned here - https://stackoverflow.com/questions/1833252/java-stanford-nlp-part-of-speech-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_vector1 = [] #POS Vector for sentece 1\n",
    "pos_vector2 = [] #POS Vector for sentece 2\n",
    "\n",
    "list_pos = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "dict_1 = {}\n",
    "dict_2 = {}\n",
    "for i in list_pos:\n",
    "    dict_1[i]=0\n",
    "    dict_2[i]=0\n",
    "\n",
    "counter = 0\n",
    "for i in range (len(tokens)):\n",
    "    if counter == 0:\n",
    "        if(tokens[i]=='.'):\n",
    "            counter += 1\n",
    "            continue\n",
    "        dict_1[pos[i][1]]+=1\n",
    "    else:\n",
    "        if(tokens[i]=='.'):\n",
    "            break\n",
    "        dict_2[pos[i][1]]+=1\n",
    "\n",
    "for i in list_pos:\n",
    "    pos_vector1.append(dict_1[i])\n",
    "    pos_vector2.append(dict_2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(pos_vector1)\n",
    "print(pos_vector2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
